{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Astradb+llangchain\\AstraRag\\DJ\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\deepa\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASTRA_DB_API_ENDPOINT= os.environ.get(\"ASTRA_DB_API_ENDPOINT\")\n",
    "ASTRA_DB_TOKEN = os.environ.get('ASTRA_DB_APPLICATION_TOKEN')\n",
    "ASTRA_DB_KEYSPACE = \"RAG\"\n",
    "TABLE_NAME = 'DJ1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 21 01:40:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   48C    P8              7W /   50W |     411MiB /   4096MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3776    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A      8968    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     12184    C+G   ...ta\\Local\\Programs\\cursor\\Cursor.exe      N/A      |\n",
      "|    0   N/A  N/A     13168    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     14300    C+G   ...b3d8bbwe\\Microsoft.Media.Player.exe      N/A      |\n",
      "|    0   N/A  N/A     15956    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     16660    C+G   ...ne\\Binaries\\Win64\\EpicWebHelper.exe      N/A      |\n",
      "|    0   N/A  N/A     18460    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     19236    C+G   ...222.0_x64__dt26b99r8h8gj\\RtkUWP.exe      N/A      |\n",
      "|    0   N/A  N/A     22476    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     23320    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     24848    C+G   ...inaries\\Win64\\EpicGamesLauncher.exe      N/A      |\n",
      "|    0   N/A  N/A     25776    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     27292    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./dj-phi-3-F16-GGUF.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dj-phi-3-F16-GGUF\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32009\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 323/32064 vs 312/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = dj-phi-3-F16-GGUF\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32009 '<|placeholder6|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7288.51 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 5024\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1884.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1884.00 MiB, K (f16):  942.00 MiB, V (f16):  942.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    21.74 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'dj-phi-3-F16-GGUF', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '32064', 'llama.context_length': '4096', 'llama.rope.dimension_count': '96', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '1', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32009', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.llamacpp import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(model_path='./dj-phi-3-F16-GGUF.F16.gguf',verbose=True,n_ctx=5000)\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"\n",
    "\n",
    "# from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# llm= HuggingFaceEndpoint(\n",
    "#     repo_id=\"deepanshdj/dj-phi-3_3.8b-16bit\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=200,\n",
    "#     do_sample=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\deepa\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Server https://api-inference.huggingface.co/models/deepanshdj/dj-phi-3_3.8b-16bit/v1/chat/completions does not seem to support chat completion. Falling back to text generation. Error: 500 Server Error: Internal Server Error for url: https://api-inference.huggingface.co/models/deepanshdj/dj-phi-3_3.8b-16bit/v1/chat/completions (Request ID: YEusADgu7HR-E0b7S7EiD)\n",
      "\n",
      "unknown error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Hugging Face is an artificial intelligence research organization that', response_metadata={'token_usage': None, 'model': '', 'finish_reason': 'unk'}, id='run-ea40c67a-5c08-4165-8755-2bb3382d6c72-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepanshdj/dj-phi-3_3.8b-16bit\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False,\n",
    ")\n",
    "llm_engine_hf = ChatHuggingFace(llm=llm)\n",
    "llm_engine_hf.invoke(\"Hugging Face is\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\Users\\deepa\\.cache\\huggingface\\hub\\models--deepanshdj--dj-phi-3-F16-GGUF\\snapshots\\bd39a7fa2284c9f5c43cd577ef6ba72e223deae5\\.\\dj-phi-3-F16-GGUF.F16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = dj-phi-3-F16-GGUF\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32009\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 323/32064 vs 312/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = dj-phi-3-F16-GGUF\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32009 '<|placeholder6|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  7288.51 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   192.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  192.00 MiB, K (f16):   96.00 MiB, V (f16):   96.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    68.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'dj-phi-3-F16-GGUF', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '32064', 'llama.context_length': '4096', 'llama.rope.dimension_count': '96', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '1', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32009', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm=Llama.from_pretrained(\n",
    "    repo_id='deepanshdj/dj-phi-3-F16-GGUF',\n",
    "    filename=\"*.gguf\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# # Define your desired data structure.\n",
    "# class Joke(BaseModel):\n",
    "#     setup: str = Field(description=\"question to set up a joke\")\n",
    "#     punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "    \n",
    "# # And a query intented to prompt a language model to populate the data structure.\n",
    "# joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# # Set up a parser + inject instructions into the prompt template.\n",
    "# parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "#     input_variables=[\"query\"],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "\n",
    "# #Chain\n",
    "# chain = prompt | llm | parser\n",
    "# #Run\n",
    "# chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1+0=1 1-1=0 0-1=-1 0-0=0 1*0=0 -1*0=0 -1/-1=1 10/5=2 20/10=2 100/50=2 9999/5000=2 99999/50000=2\n",
      "The 4 numbers to the left of the decimal point have no effect on the answer.\n",
      "After that, every\n"
     ]
    }
   ],
   "source": [
    "# print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init LLM and embeddings model\n",
    "# #embeddings = OpenAIEmbeddings() #1536\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     model_kwargs={'device': 'cpu'},\n",
    "#     encode_kwargs={'normalize_embeddings': False},\n",
    "# )\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=HF_TOKEN, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Astradb+llangchain\\AstraRag\\DJ\\lib\\site-packages\\pydantic\\_internal\\_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name = \"deepanshdj/dj-phi-3_3.8b-16bit\",\n",
    "    api_key = HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\nmax_new_tokens\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[1;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceHub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeepanshdj/dj-phi-3_3.8b-16bit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhuggingfacehub_api_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Astradb+llangchain\\AstraRag\\DJ\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Astradb+llangchain\\AstraRag\\DJ\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\nmax_new_tokens\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='deepanshdj/dj-phi-3_3.8b-16bit', huggingfacehub_api_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = AstraDBVectorStore(\n",
    "    embedding=embeddings,\n",
    "    collection_name=TABLE_NAME,\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_TOKEN,\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='efficacious, lofty, devoted to his duties, wealthy and will serve a king. \\n\\nIf born in the second decanate of Aquarius, one will be a miser, be skillful, sweet, fair \\n\\ncomplexioned, tawny and broad eyed, will earn wealth through amusement, will speak \\n\\nunhesitatingly, be intelligent and will be endowed with many friends. \\n\\nIf born in the third decanate, one will be tall in stature, crafty, valorous, emaciated, short-armed, \\n\\nbe endowed with sons and wealth, hard-hearted, will tell many lies, be crooked in nature, will \\n\\nhave afflicted eyes and will have knowledge of sexology. \\n\\n34-36. DECANATES IN PISCES. \\n\\nIf the Ascendant is in the first decanate of Pisces, one will have honey coloured and tawny eyes, \\n\\nbe fair in complexion, very learned, will perform virtuous deeds, be happy, will undertake trips \\n\\nby sea etc. and be modest. \\n\\nIf it be the second decanate of Pisces, one will be skillful in serving the fair sex, will eat purified \\n\\nfood, will enjoy others wealth, be sensuous, dear to females and virtuous people and be an \\n\\nexcellent person.  \\n\\nIf born in the last decanate of Pisces one will be dark in complexion, be skillful in arts, will have \\n\\nwide feet, will be generous with friends and will consume purified food and drinks.  \\n\\n37. Thus spoke of the decanates the preceptors well-versed in that branch. Should the Sign \\n\\nholding the decanate be strong, or be aspected by its Lord, these effects will mature in full. \\n\\nThus ends the 50th Ch. entitled EFFECTS OF DECANATES (lost horoscopy) in Kalyana Varmas \\n\\nSARAVALI. \\n\\n \\n\\n \\n\\n\\n\\n 186 \\n\\nChapter 51. \\n  \\n\\nEffects Of Navansas \\n\\n1.  A natives complexion, disposition and appearance can be deduced based on the rising \\n\\nNavansa at birth. The effects being stated infra will depend on the stronger of the two, viz. the \\n\\nMoons Navansa dispositor and the Navansa Lagna Lord. \\n\\n  \\n\\n2-10. EFFECTS OF NAVANSAS IN ARIES. \\n\\nFollowing are the effects of births in the nine Navansas in Aries Ascendant: Effects of Aries', metadata={'file_path': 'C:\\\\Users\\\\deepa\\\\AppData\\\\Local\\\\langflow\\\\langflow\\\\Cache\\\\70e05501-3d7d-4f71-b4d1-84a025e6af69\\\\datanewKalyana_Varmas_Saravali_JYOTISH_VEDIC_ASTROLOGY.txt'}),\n",
       " Document(page_content='complexion akin to copper, be radiant and interested in travels.  \\nIf born in the second half of Aquarius, one will have copper-brilliant eyes, be emaciated, firm, \\nvery insignificant in appearance, indolent, not outspoken, very dejected, miserly and very \\ncrafty.  \\n23-24. HORAS IN PISCES. One born in the first 15 degrees of Pisces will be short-statured, be \\nendowed with broad and beautiful body, a large fore face, large face and broad chest, be dear \\nto women, be very famous, skillful and valorous.  \\nIf born in the second Hora of Pisces, one will be liberal, will have an elevated nose, be skillful, \\nintelligent, will possess charming eyes be dear to king and will speak affably.  \\n25. The (good effects) due to a Hora will come to pass in full measure, if either the Sun, or the \\nMoon is strong in aspect to the Ascendant Lord, or, if the Ascendant Lord is himself in an Angle. \\n \\nThus ends the 49th Ch. entitled \"EFFECTS OF HORAS (lost horoscopy)\" in Kalyana Varma\\'s \\nSaravali. \\nCh. 50. Effects Of Decanates \\n \\n1-3. DECANATES IN ARIES. One born in the first decanate of Aries will be charitable, be a thief, \\nbe splendourous, will have checkered prosperity, be fierce in battle (or in quarrels), pleasing in \\nappearance and will inflict punishment on relatives.  \\nIf born in the second decanate of Aries, he will be attached to females, will wander, be \\ninterested in carnal pleasures and music, be intelligent, be endowed with friends and wealth, be \\ngood-looking and will have an eye on women\\'s property.  \\nIf born in the third decanate of Aries, he will be virtuous, will fault others, be fickle minded, \\nmighty, will serve a king, be attached to his own men, be very righteous and honourable, but \\nnot learned.  \\n4-6. DECANATES IN TAURUS. If the birth be in the first decanate of Taurus, one will be fond of \\nfood and drinks, be tormented due to separation of his wife, endowed with robes and \\nornaments and will act according to the desires of his wife.', metadata={'file_path': 'C:\\\\Users\\\\deepa\\\\AppData\\\\Local\\\\langflow\\\\langflow\\\\Cache\\\\70e05501-3d7d-4f71-b4d1-84a025e6af69\\\\datanewAstrology.txt'}),\n",
       " Document(page_content='34. The third Decanate in Aquarius is a man with various weapons wearing a garland of golden \\nMoons. His shape is boar-like, his form frightful. Producing red (?) in Malaya, he is an ascetic \\nwhose hair is reddish-brown like a monkey’s. \\n\\n35. The first Decanate in Pisces is a woman with a beautiful body whose eyes are expansive and long. \\nHer body is adorned with silk and gold. She stands by the Great Sea, which she has crossed in a \\nboat for the sake of a heap of jewels. \\n\\n36. The second Decanate in Pisces is a woman dreadful in strife, the foremost one. She is fierce, and \\nhas no clothes; her color is white, red, and black. Her garments and ornaments are destroyed; \\ndesiring clothes, she shouts out. \\n\\n37. The third Decanate in Pisces is a woman whose hair has been loosened and who wears ornaments \\nbearing the emblem of the Abhiras. She shrieks as she is frightened. She stands in the water \\nadorned by troops of spirits having the shapes of jackals, cats, and boars. \\n\\n38. These thirds of the signs which are called Drekkanas together with the natures that accompany \\neach, to which much thought has been given, have been thus described by the great Greek \\nmasters who know the meanings, properties, and traditions of horoscopy. \\n\\n39. Because of its doubtfulness, this pictorial representation is (to be) combined with (the effects of) \\nthe lords of the navamsas, the navamsas themselves, and the aspects of the planets; it is useful \\nbecause it exemplifies the many forms, natures, and distinguishing marks in the world. \\n\\n40. Whatever characteristics of a planet have been described with regard to the signs, the navamsas, \\nand times, or whatever causal natures, their entire effect is in full force in all actions for \\nwhatsoever purpose. \\n\\n41. What are the form, nature, quality, and distinguishing mark of (each) planet and sign has been \\nsaid previously; the form which arises from the changes due to their mutual combinations in order \\nis to be determined by a wise man.', metadata={'file_path': 'C:\\\\Users\\\\deepa\\\\AppData\\\\Local\\\\langflow\\\\langflow\\\\Cache\\\\70e05501-3d7d-4f71-b4d1-84a025e6af69\\\\datanewYAVANAJATAKA.txt'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"pisces?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#llm = ChatOllama(model=\"deepudj/dj-phi3\", format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"pisces person\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Deepansh Jha, an astrology assistant. I can answer questions related to horoscopes and astrological predictions based on the context provided by you. If you have any specific question or need guidance in a particular area of astrology, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an astrology assistant Deepansh Jha for question-answering astrology tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"deepudj/dj-phi3\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"Who are you?\"\n",
    "docs = retriever.invoke(question)\n",
    "formatted_docs = format_docs(docs)  # Use the format_docs function to format the documents\n",
    "generation = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Astrology is a science that studies the influence of celestsial bodies on human life and events. It uses various tools such as horoscopes, planetary positions, and other astrological indicators to predict future outcomes based on past observations. \n",
      "2) The process of astrology involves studying the movements and positions of stars and planets in relation to Earth's position at a given time. Astrologers use this information to create horoscopes, which are charts that represent the sky as seen from a specific location on Earth at a particular moment in time. These horoscopes can be used to predict future events or gain insight into an individual's personality and life path. \n",
      "3) Astrology is based on the belief that celestial bodies have a direct influence on human behavior, emotions, and destiny. It is considered a pseudoscience by many because it lacks empirical evidence to support its claims. However, astrology has been practiced for thousands of years in various cultures around the world as a way to understand and predict natural phenomena. \n",
      "4) Astrologers use a variety of tools and techniques to interpret celestial movements and make predictions about future events. These include studying planetary positions, analyzing zodiac signs, and using other astrological indicators such as lunar cycles and solar eclipses. They may also consult ancient texts or rely on their own intuition and experience to guide their interpretations. \n",
      "5) Astrology is often used for personal guidance and self-discovery, but it can also be applied to business decisions, relationships, and other aspects of life. Some people believe that astrological predictions are accurate and reliable, while others view them as mere entertainment or superstition. Ultranceastly, the practice of astrology is a complex and multifaceted subject with many different interpretations and beliefs.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"How astrology works?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. A man born in the first Hora of Aquarius has red eyes which are wounded, obstructed and torn; he is a weak cripple, black and disconsolate, a lazy, dishonest and wretched rogue.\n",
      "2. One born in the second Hora of Pisces is liberal, will have an elevated nose, be intelligent, skillful, possess charming eyes and will speak affably to the king.\n",
      "3. If the birth is in the first decanate of Taurus, one will be fond of food and drinks, tormented due to separation from his wife, endowed with robes and ornaments, and act according to her desires.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"What can you tell about pisces who is thin and short?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compatibility of Pisces and Taurus is generally considered to be good, as both are mutable signs that share a common element of water. Additionally, Taurus rules Cancer in the zodiac, which is another beneficial planetary relationship for this pairing. However, it's recommended to avoid the first three degrees (0-30°) and 14th lunar day as they may pose challenges for certain aspects of their connection. It's also suggested that both signs should take care not to rely too much on each other, but instead encourage individual growth. Overall, this pairing has the potential to be very compatible with mutual respect and open communication.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"what is a compatibility of pisces and taurus?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is the significance of Bhukti in astrology?\n",
      "2. How can one use the information from Stanza 19 to their advantage in national or mundane astrology?\n",
      "3. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "4. What is the significance of the 12 Bhavas in relation to Bhukti and Raja Prasna?\n",
      "5. How can one interpret the information from Stanza 19 in a practical way for their own life or career?\n",
      "6. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "7. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "8. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "9. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "10. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "11. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "12. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "13. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "14. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "15. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "16. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "17. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "18. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "19. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "20. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "21. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "22. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "23. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "24. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "25. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "26. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "27. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "28. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "29. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "30. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "31. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "32. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "33. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "34. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "35. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "36. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "37. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "38. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "39. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "40. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "41. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "42. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "43. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "44. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "45. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "46. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "47. How can one use the information from Stanza 19 to gain insight into their personal relationships or family dynamics?\n",
      "48. Can you provide an example of how a specific question related to eating food (Bhukti) might have been used in ancient times and its relevance today?\n",
      "49. How can one use the information from Stanza 19 to gain insight into their career or professional life?\n",
      "50. Can you explain how the different houses (Lagna, Treasure, Vehicles, Diplomacy, Enemy, Communications, Longevity, Mind of King, Trade and Commerce) relate to Bhukti and Raja Prasna in more detail?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"wer bist du?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you're asking about the various factors to consider in Hindu astrology for determining the compatibility of a couple, specifically the Rasi Guna Yoga and Graha Guna Yoga.\n",
      "\n",
      "To calculate the Rasi Guna Yoga, you multiply the bindus (or points) in each sign occupied by the couple's zodiac signs by their respective zodiacal factors. The sum of these products gives you the Rasi Guna Yoga or the sum of the zodiacal factors.\n",
      "\n",
      "For the Graha Guna Yoga, you multiply the bindus in the signs occupied by the planets (Sun, Moon, Mars, Mercury, Jupiter, Venus, and Saturn) by their respective planetary factors. The total of these products gives you the Graha Guna Yoga or the sum of the planetary factors.\n",
      "\n",
      "When you add the Rasi Guna Yoga and Graha Guna Yoga together, you get the Suddha Pinda.\n",
      "\n",
      "Additionally, there are other factors to consider such as:\n",
      "\n",
      "1. Stanza 81: The asterism that appears when Saturn transits the 9th from the Sun in his own Ashtakavarga (after reduction) is said to indicate the death of the father.\n",
      "2. Stanza 82: The relieving factors for a couple are determined by multiplying the total figures in the Sun's Ashtakavarga (after reduction) by the figures in the 9th from the Sun, and then dividing the product by 27.\n",
      "\n",
      "There are also other factors to consider such as Varna (the degree of spiritual or ego development), Nadi Kuta (the pulse or nervous energy indicating physiological and hereditary factors), and Vedhakuta (the absence of which indicates a prohibited pair).\n",
      "\n",
      "Please let me know if you have any further questions or if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"How is the compatibility of pisces and cancer?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
